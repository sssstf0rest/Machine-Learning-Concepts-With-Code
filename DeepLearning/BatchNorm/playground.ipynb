{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batch Normalization (from scratch)\n",
        "This notebook walks through *what BatchNorm does* and how the provided `Layer_BatchNormalization` works.\n",
        "\n",
        "We focus on the most common 2D case: inputs shaped `(batch, features)` (e.g., activations of an MLP layer).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Why normalization helps\n",
        "BatchNorm stabilizes training by normalizing activations using **batch statistics** (mean/variance), then learning an affine re-scale (`gamma`) and shift (`beta`).\n",
        "\n",
        "Training-time (per feature):\n",
        "- `mu = mean(x)` over the batch\n",
        "- `var = var(x)` over the batch\n",
        "- `x_hat = (x - mu) / sqrt(var + eps)`\n",
        "- `y = gamma * x_hat + beta`\n",
        "\n",
        "Inference-time: use **running mean/var** accumulated during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training output mean (per feature): [-8.50014503e-17  5.02527707e-17 -1.82145965e-16 -2.42861287e-17\n",
            "  1.67400815e-16 -2.94035629e-16  8.67361738e-18 -1.63064007e-16\n",
            "  1.35308431e-16  5.55111512e-17]\n",
            "training output var  (per feature): [0.99999963 0.99999956 0.99999955 0.99999961 0.99999962 0.99999954\n",
            " 0.99999954 0.99999965 0.99999962 0.99999951]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from batchnorm import Layer_BatchNormalization\n",
        "\n",
        "np.random.seed(0)\n",
        "x = np.random.randn(64, 10) * 5 + 3  # deliberately non-zero mean/variance\n",
        "bn = Layer_BatchNormalization(n_features=10)\n",
        "\n",
        "y = bn.forward(x, training=True)\n",
        "print('training output mean (per feature):', y.mean(axis=0))\n",
        "print('training output var  (per feature):', y.var(axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Running statistics\n",
        "During training, we maintain exponentially-decayed running estimates.\n",
        "These are used when `training=False`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running_mean: [[0.86773458 0.8619657  0.83573998 0.79636782 0.86087148 0.86764978\n",
            "  0.9859476  0.91650534 0.92824561 0.89680214]]\n",
            "running_var : [[3.90176783 3.98873478 3.92112058 3.78750281 3.84205094 3.80501912\n",
            "  3.70500786 3.88168344 4.00293304 3.61602823]]\n",
            "inference output shape: (8, 10)\n"
          ]
        }
      ],
      "source": [
        "# Run multiple batches to update running stats\n",
        "for _ in range(20):\n",
        "    xb = np.random.randn(64, 10) * 2 + 1\n",
        "    _ = bn.forward(xb, training=True)\n",
        "\n",
        "print('running_mean:', bn.running_mean)\n",
        "print('running_var :', bn.running_var)\n",
        "\n",
        "# Inference: normalization uses running stats\n",
        "x_test = np.random.randn(8, 10) * 2 + 1\n",
        "y_test = bn.forward(x_test, training=False)\n",
        "print('inference output shape:', y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Backprop intuition\n",
        "BatchNormâ€™s backward pass has three learnable gradients:\n",
        "- `dgamma` and `dbeta` are straightforward\n",
        "- `dinputs` is trickier because `mu` and `var` depend on all samples in the batch\n",
        "\n",
        "The implementation in `batchnorm.py` uses the standard analytic derivative.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Quick check: run the gradient test\n",
        "The repo includes a finite-difference check to validate the backward pass.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In terminal, run:\n",
        "# python -m BatchNorm.test_batchnorm\n",
        "# (This notebook doesn't auto-run it.)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "title": "BatchNorm Playground"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
