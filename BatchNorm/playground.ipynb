{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Batch Normalization (from scratch)\nThis notebook walks through *what BatchNorm does* and how the provided `Layer_BatchNormalization` works.\n\nWe focus on the most common 2D case: inputs shaped `(batch, features)` (e.g., activations of an MLP layer).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Why normalization helps\nBatchNorm stabilizes training by normalizing activations using **batch statistics** (mean/variance), then learning an affine re-scale (`gamma`) and shift (`beta`).\n\nTraining-time (per feature):\n- `mu = mean(x)` over the batch\n- `var = var(x)` over the batch\n- `x_hat = (x - mu) / sqrt(var + eps)`\n- `y = gamma * x_hat + beta`\n\nInference-time: use **running mean/var** accumulated during training.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nfrom BatchNorm.batchnorm import Layer_BatchNormalization\n\nnp.random.seed(0)\nx = np.random.randn(64, 10) * 5 + 3  # deliberately non-zero mean/variance\nbn = Layer_BatchNormalization(n_features=10)\n\ny = bn.forward(x, training=True)\nprint('training output mean (per feature):', y.mean(axis=0))\nprint('training output var  (per feature):', y.var(axis=0))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Running statistics\nDuring training, we maintain exponentially-decayed running estimates.\nThese are used when `training=False`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Run multiple batches to update running stats\nfor _ in range(20):\n    xb = np.random.randn(64, 10) * 2 + 1\n    _ = bn.forward(xb, training=True)\n\nprint('running_mean:', bn.running_mean)\nprint('running_var :', bn.running_var)\n\n# Inference: normalization uses running stats\nx_test = np.random.randn(8, 10) * 2 + 1\ny_test = bn.forward(x_test, training=False)\nprint('inference output shape:', y_test.shape)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Backprop intuition\nBatchNormâ€™s backward pass has three learnable gradients:\n- `dgamma` and `dbeta` are straightforward\n- `dinputs` is trickier because `mu` and `var` depend on all samples in the batch\n\nThe implementation in `batchnorm.py` uses the standard analytic derivative.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Quick check: run the gradient test\nThe repo includes a finite-difference check to validate the backward pass.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# In terminal, run:\n# python -m BatchNorm.test_batchnorm\n# (This notebook doesn't auto-run it.)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "title": "BatchNorm Playground"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
