{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Layer Normalization (from scratch)\nThis notebook explains LayerNorm and how `Layer_LayerNormalization` works.\n\nLayerNorm normalizes **within each sample** (across features), so it does NOT need running statistics.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Formula\nFor each sample `i` (row vector):\n- `mu_i = mean(x_i)` across features\n- `var_i = var(x_i)` across features\n- `x_hat_i = (x_i - mu_i) / sqrt(var_i + eps)`\n- `y_i = gamma * x_hat_i + beta`\n\nCompared to BatchNorm, LayerNorm behaves consistently between train and eval.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nfrom LayerNorm.layernorm import Layer_LayerNormalization\n\nnp.random.seed(0)\nx = np.random.randn(4, 6) * 4 + 10\nln = Layer_LayerNormalization(n_features=6)\ny = ln.forward(x)\n\nprint('per-sample mean:', y.mean(axis=1))\nprint('per-sample var :', y.var(axis=1))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Backward pass intuition\nLayerNorm's backward is similar in spirit to BatchNorm but computed per-sample.\nThe provided `layernorm.py` implements a closed-form gradient.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Quick check: run the gradient test\nThe repo includes a finite-difference gradient check.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# In terminal, run:\n# python -m LayerNorm.test_layernorm\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "title": "LayerNorm Playground"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
