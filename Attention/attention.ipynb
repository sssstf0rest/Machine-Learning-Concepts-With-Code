{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f951b1cc",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3c8ebd",
   "metadata": {},
   "source": [
    "## Problem Setup and Input Representation\n",
    "We start with a token sequence $x = (x_1, x_2, \\dots, x_T)$, where each token is embedded into $e_i \\in \\mathbb{R}^{d_{\\text{model}}}$.  \n",
    "\n",
    "Token embeddings come from a matrix $E \\in \\mathbb{R}^{|V| \\times d_{\\text{model}}}$ via $e_i = E[x_i]$. Because Transformers are non-recurrent, positional encodings inject order: $z_i^{(0)} = e_i + p_i$, where $p_i$ can be sinusoidal or learned.  \n",
    "\n",
    "For sinusoidal encodings: $p_{i,2k} = \\sin\\left(\\tfrac{i}{10000^{2k/d}}\\right)$ and $p_{i,2k+1} = \\cos\\left(\\tfrac{i}{10000^{2k/d}}\\right)$. This notebook then walks through how those embeddings are processed by attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3979af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and utilities\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "    \"\"\"Numerically-stable softmax.\"\"\"\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def pretty(a: np.ndarray, name: str, decimals: int = 4) -> None:\n",
    "    \"\"\"Pretty-print a numpy array with a label.\"\"\"\n",
    "    np.set_printoptions(precision=decimals, suppress=True)\n",
    "    print(f\"\\n{name} (shape={a.shape}):\\n{a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b61ba3b",
   "metadata": {},
   "source": [
    "## Core Attention Math\n",
    "Container for intermediate tensors and the core operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34e793c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AttentionOutputs:\n",
    "    \"\"\"Container holding intermediate values useful for explanation/debugging.\"\"\"\n",
    "\n",
    "    Q: np.ndarray\n",
    "    K: np.ndarray\n",
    "    V: np.ndarray\n",
    "    scores: np.ndarray\n",
    "    attn_weights: np.ndarray\n",
    "    output: np.ndarray\n",
    "\n",
    "\n",
    "def make_qkv(\n",
    "    X: np.ndarray,\n",
    "    W_Q: np.ndarray,\n",
    "    W_K: np.ndarray,\n",
    "    W_V: np.ndarray,\n",
    " ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Project input embeddings X to Q, K, V.\"\"\"\n",
    "    Q = X @ W_Q\n",
    "    K = X @ W_K\n",
    "    V = X @ W_V\n",
    "    return Q, K, V\n",
    "\n",
    "\n",
    "def apply_attention_mask(scores: np.ndarray, mask: Optional[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"Apply an additive mask to attention scores (1=mask).\"\"\"\n",
    "    if mask is None:\n",
    "        return scores\n",
    "\n",
    "    if mask.shape != scores.shape:\n",
    "        raise ValueError(f\"Mask shape {mask.shape} must match scores shape {scores.shape}\")\n",
    "\n",
    "    neg_inf = -1e9\n",
    "    masked_scores = scores + (mask.astype(np.float32) * neg_inf)\n",
    "    return masked_scores\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(\n",
    "    Q: np.ndarray,\n",
    "    K: np.ndarray,\n",
    "    V: np.ndarray,\n",
    "    mask: Optional[np.ndarray] = None,\n",
    ") -> AttentionOutputs:\n",
    "    \"\"\"Compute scaled dot-product attention.\"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = (Q @ K.T) / np.sqrt(d_k)\n",
    "    scores_masked = apply_attention_mask(scores, mask)\n",
    "    attn_weights = softmax(scores_masked, axis=-1)\n",
    "    output = attn_weights @ V\n",
    "    return AttentionOutputs(Q=Q, K=K, V=V, scores=scores_masked, attn_weights=attn_weights, output=output)\n",
    "\n",
    "\n",
    "def causal_mask(T: int) -> np.ndarray:\n",
    "    \"\"\"Causal mask: disallow looking ahead (upper triangle).\"\"\"\n",
    "    return np.triu(np.ones((T, T), dtype=np.int32), k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae8557",
   "metadata": {},
   "source": [
    "## Demo Setup\n",
    "Tiny 3-token example with human-readable numbers (d_model=4, d_k=2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a19d0a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0: Input embeddings X (shape=(3, 4)):\n",
      "[[1. 0. 1. 0.]\n",
      " [0. 2. 0. 2.]\n",
      " [1. 1. 0. 0.]]\n",
      "\n",
      "W_Q (shape=(4, 2)):\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "W_K (shape=(4, 2)):\n",
      "[[1. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "\n",
      "W_V (shape=(4, 2)):\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Example data and projection matrices\n",
    "X = np.array(\n",
    "    [\n",
    "        [1.0, 0.0, 1.0, 0.0],  # token 1\n",
    "        [0.0, 2.0, 0.0, 2.0],  # token 2\n",
    "        [1.0, 1.0, 0.0, 0.0],  # token 3\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    " )\n",
    "\n",
    "W_Q = np.array(\n",
    "    [\n",
    "        [1.0, 0.0],\n",
    "        [0.0, 1.0],\n",
    "        [1.0, 0.0],\n",
    "        [0.0, 1.0],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    " )\n",
    "\n",
    "W_K = np.array(\n",
    "    [\n",
    "        [1.0, 1.0],\n",
    "        [1.0, 0.0],\n",
    "        [0.0, 1.0],\n",
    "        [1.0, 0.0],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    " )\n",
    "\n",
    "W_V = np.array(\n",
    "    [\n",
    "        [1.0, 0.0],\n",
    "        [0.0, 1.0],\n",
    "        [1.0, 1.0],\n",
    "        [0.0, 1.0],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    " )\n",
    "\n",
    "pretty(X, \"Step 0: Input embeddings X\")\n",
    "pretty(W_Q, \"W_Q\")\n",
    "pretty(W_K, \"W_K\")\n",
    "pretty(W_V, \"W_V\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a40f8fc",
   "metadata": {},
   "source": [
    "## Step 1 — Project to Q, K, V\n",
    "Compute queries/keys/values via learned projections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a739c0",
   "metadata": {},
   "source": [
    "### Attention Math Used in Steps 1–3\n",
    "For each position $i$ we project $z_i$ into $q_i = W_Q z_i$, $k_i = W_K z_i$, $v_i = W_V z_i$, with $W_Q, W_K, W_V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$. Stacked: $Q = Z W_Q$, $K = Z W_K$, $V = Z W_V$.  \n",
    "\n",
    "Scaled dot-product attention computes $A = \\tfrac{Q K^\\top}{\\sqrt{d_k}}$, applies a row-wise softmax $\\alpha = \\text{softmax}(A)$, and produces $\\text{Attention}(Q,K,V) = \\alpha V$.  \n",
    "\n",
    "For causal/decoder use, we mask future tokens so that $A_{ij} = \\tfrac{q_i \\cdot k_j}{\\sqrt{d_k}}$ if $j \\le i$ and $-\\infty$ otherwise, ensuring $P(x_i \\mid x_1, \\dots, x_{i-1})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e1b2793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Queries Q = X @ W_Q (shape=(3, 2)):\n",
      "[[2. 0.]\n",
      " [0. 4.]\n",
      " [1. 1.]]\n",
      "\n",
      "Step 1: Keys    K = X @ W_K (shape=(3, 2)):\n",
      "[[1. 2.]\n",
      " [4. 0.]\n",
      " [2. 1.]]\n",
      "\n",
      "Step 1: Values  V = X @ W_V (shape=(3, 2)):\n",
      "[[2. 1.]\n",
      " [0. 4.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "Q, K, V = make_qkv(X, W_Q, W_K, W_V)\n",
    "\n",
    "pretty(Q, \"Step 1: Queries Q = X @ W_Q\")\n",
    "pretty(K, \"Step 1: Keys    K = X @ W_K\")\n",
    "pretty(V, \"Step 1: Values  V = X @ W_V\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a09fc6",
   "metadata": {},
   "source": [
    "## Step 2 — Compute Attention Scores\n",
    "Calculate scaled dot products between queries and keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4276c9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Raw scores = (Q @ K^T) / sqrt(d_k=2) (shape=(3, 3)):\n",
      "[[1.4142 5.6569 2.8284]\n",
      " [5.6569 0.     2.8284]\n",
      " [2.1213 2.8284 2.1213]]\n",
      "d_k:  2\n"
     ]
    }
   ],
   "source": [
    "d_k = Q.shape[-1]\n",
    "raw_scores = (Q @ K.T) / np.sqrt(d_k)\n",
    "pretty(raw_scores, f\"Step 2: Raw scores = (Q @ K^T) / sqrt(d_k={d_k})\")\n",
    "print(\"d_k: \", d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2b42b",
   "metadata": {},
   "source": [
    "## Step 3 — Apply Causal Mask\n",
    "Disallow attending to future positions before softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb1d017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Causal mask (1=masked/future, 0=allowed) (shape=(3, 3)):\n",
      "[[0 1 1]\n",
      " [0 0 1]\n",
      " [0 0 0]]\n",
      "\n",
      "Step 3: Masked scores (future positions -> very negative) (shape=(3, 3)):\n",
      "[[ 1.4142e+00 -1.0000e+09 -1.0000e+09]\n",
      " [ 5.6569e+00  0.0000e+00 -1.0000e+09]\n",
      " [ 2.1213e+00  2.8284e+00  2.1213e+00]]\n"
     ]
    }
   ],
   "source": [
    "T = X.shape[0]\n",
    "mask = causal_mask(T)\n",
    "pretty(mask, \"Step 3: Causal mask (1=masked/future, 0=allowed)\")\n",
    "\n",
    "masked_scores = apply_attention_mask(raw_scores, mask)\n",
    "pretty(masked_scores, \"Step 3: Masked scores (future positions -> very negative)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcedaa7a",
   "metadata": {},
   "source": [
    "## Step 4 — Softmax to Get Attention Weights\n",
    "Convert masked logits into probabilities and verify rows sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef350b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Attention weights = softmax(masked_scores) (shape=(3, 3)):\n",
      "[[1.     0.     0.    ]\n",
      " [0.9965 0.0035 0.    ]\n",
      " [0.2483 0.5035 0.2483]]\n",
      "\n",
      "(Check) Row sums of attention weights (shape=(3,)):\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "weights = softmax(masked_scores, axis=-1)\n",
    "pretty(weights, \"Step 4: Attention weights = softmax(masked_scores)\")\n",
    "\n",
    "row_sums = weights.sum(axis=-1)\n",
    "pretty(row_sums, \"(Check) Row sums of attention weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b824e7db",
   "metadata": {},
   "source": [
    "## Step 5 — Weighted Sum of Values\n",
    "Multiply weights by values to produce the attended output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f281d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Attention output = weights @ V (shape=(3, 2)):\n",
      "[[2.     1.    ]\n",
      " [1.993  1.0104]\n",
      " [0.7448 2.5105]]\n"
     ]
    }
   ],
   "source": [
    "output = weights @ V\n",
    "pretty(output, \"Step 5: Attention output = weights @ V\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6cc217",
   "metadata": {},
   "source": [
    "## Step 6 — One-Call API Check\n",
    "Call the packaged `scaled_dot_product_attention` and confirm it matches the manual steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ac4de",
   "metadata": {},
   "source": [
    "## Beyond This Single-Head Demo\n",
    "**Multi-Head Attention:**  \n",
    "Use $h$ heads: $\\text{head}_j = \\text{Attention}(Q W_Q^{(j)}, K W_K^{(j)}, V W_V^{(j)})$, then $\\text{MHA}(Z) = \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h) W_O$ with $W_O \\in \\mathbb{R}^{(h d_k) \\times d_{\\text{model}}}$. Multiple heads capture syntax, coreference, and long-range cues.  \n",
    "\n",
    "**Feed-Forward Network (per position):**  \n",
    "$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$, with $W_1 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{ff}}}$, $W_2 \\in \\mathbb{R}^{d_{\\text{ff}} \\times d_{\\text{model}}}$, typically $d_{\\text{ff}} \\approx 4 d_{\\text{model}}$.  \n",
    "\n",
    "**Residuals + LayerNorm:**  \n",
    "Each block: $\\tilde{Z} = \\text{LayerNorm}(Z + \\text{MHA}(Z))$ then $Z' = \\text{LayerNorm}(\\tilde{Z} + \\text{FFN}(\\tilde{Z}))$. Residuals aid gradients; LayerNorm stabilizes.  \n",
    "\n",
    "**Full Stack:**  \n",
    "For $L$ layers: $Z^{(l+1)} = \\text{TransformerBlock}(Z^{(l)})$, starting from $Z^{(0)} = E(x) + P$, ending with $H = Z^{(L)}$.  \n",
    "\n",
    "**Output Layer (LM):**  \n",
    "Per time step: $\\hat{y}_t = \\text{softmax}(H_t W_{\\text{out}} + b)$.  \n",
    "\n",
    "**Training Objective (GPT-style):**  \n",
    "Autoregressive MLE: $\\mathcal{L} = - \\sum_{t=1}^{T} \\log P(x_t \\mid x_1, \\dots, x_{t-1})$ (cross-entropy). Even this simple objective yields learned syntax, semantics, reasoning, and instruction-following.  \n",
    "\n",
    "**Why Scaling Works:**  \n",
    "No recurrence → parallelism; attention gives global receptive field; adding depth/width/data yields emergent abilities—hence non-linear jumps from GPT-2 to GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "368ec8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. The outputs from manual steps match the function output.\n"
     ]
    }
   ],
   "source": [
    "out = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
    "\n",
    "assert np.allclose(out.scores, masked_scores)\n",
    "assert np.allclose(out.attn_weights, weights)\n",
    "assert np.allclose(out.output, output)\n",
    "print(\"\\n✅ Done. The outputs from manual steps match the function output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f1392",
   "metadata": {},
   "source": [
    "## Interpretation Example\n",
    "Inspect one token's attention distribution over the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40582b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interpretation example:\n",
      "Token 3 attention weights over tokens 1..3: [0.2483 0.5035 0.2483]\n",
      "This means token 3 forms its representation as a weighted sum of V1, V2, V3.\n"
     ]
    }
   ],
   "source": [
    "i = 2  # token 3 (0-indexed)\n",
    "print(\"\\nInterpretation example:\")\n",
    "print(f\"Token {i+1} attention weights over tokens 1..{T}: {weights[i]}\")\n",
    "print(\"This means token 3 forms its representation as a weighted sum of V1, V2, V3.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
