{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ResNet-18 (MNIST) — built from this repo's CNN + DNN blocks\n",
        "\n",
        "This notebook explains the **structure** of a ResNet-18 style network and how the provided implementation is assembled using:\n",
        "- `CNN.layers.Layer_Conv2D_Im2Col` (convolution)\n",
        "- `CNN.layers.Layer_MaxPool2D` / `Layer_Flatten` (basic CNN utilities)\n",
        "- `DNN` modules (Dense, activations, loss, optimizers)\n",
        "\n",
        "**Important constraint:** this implementation uses **NumPy only** — no `torch` imports.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Residual learning (the key idea)\n",
        "A residual block computes:\n",
        "\n",
        "\\[ y = F(x) + x \\] \n",
        "\n",
        "If shapes differ (e.g., downsampling), the shortcut becomes:\n",
        "\n",
        "\\[ y = F(x) + W_s x \\] \n",
        "\n",
        "This makes deep networks easier to optimize because each block only needs to learn a *residual correction*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) What a BasicBlock contains\n",
        "In ResNet-18, a BasicBlock is:\n",
        "- `conv3x3 -> bn -> relu`\n",
        "- `conv3x3 -> bn`\n",
        "- `+ shortcut`\n",
        "- `relu`\n",
        "\n",
        "In our code: `BasicBlock` is implemented in `ResNet/resnet18_numpy.py` and internally uses `Layer_Conv2D_Im2Col` + `Activation_ReLU` + a scratch `Layer_BatchNorm2D`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 16, 28, 28)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from resnet18_numpy import BasicBlock\n",
        "\n",
        "np.random.seed(0)\n",
        "x = np.random.randn(2, 16, 28, 28).astype(np.float32)\n",
        "blk = BasicBlock(in_ch=16, out_ch=16, stride=1)\n",
        "blk.forward(x, training=True)\n",
        "blk.output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) BatchNorm2D (scratch)\n",
        "For conv tensors `(N, C, H, W)`, BatchNorm2D normalizes per-channel over `(N, H, W)`.\n",
        "\n",
        "Training uses batch mean/var; inference uses running mean/var.\n",
        "We expose `weights`/`biases` for compatibility with the repo's optimizers (they correspond to `gamma`/`beta`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([-2.4328426e-09,  3.0410534e-09, -2.4328426e-09,  0.0000000e+00,\n",
              "         4.8656852e-09], dtype=float32),\n",
              " array([0.99998975, 0.9999893 , 0.999989  , 0.9999898 , 0.9999895 ],\n",
              "       dtype=float32))"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ResNet.resnet18_numpy import Layer_BatchNorm2D\n",
        "\n",
        "bn = Layer_BatchNorm2D(n_channels=16)\n",
        "bn.forward(x, training=True)\n",
        "bn.output.mean(axis=(0,2,3))[:5], bn.output.var(axis=(0,2,3))[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) The ResNet-18 style MNIST model\n",
        "We adapt the stem for MNIST (28×28): use a 3×3 conv instead of the original 7×7 + maxpool.\n",
        "Then use 4 stages with [2,2,2,2] blocks and channels [16,32,64,128].\n",
        "Finally: GlobalAveragePooling + Dense to 10 classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4, 10)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ResNet.resnet18_numpy import ResNet18MNIST\n",
        "\n",
        "model = ResNet18MNIST(num_classes=10)\n",
        "x0 = np.random.randn(4, 1, 28, 28).astype(np.float32)\n",
        "model.forward(x0, training=True)\n",
        "model.output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Training entrypoint\n",
        "Use `ResNet/train_mnist.py`. It loads MNIST via a NumPy-only downloader (`mnist_data.py`) and trains with the repo's cross-entropy + Adam optimizer.\n",
        "\n",
        "For a quick CPU run:\n",
        "```bash\n",
        "python3 -m ResNet.train_mnist --epochs 1 --subset 5000 --batch-size 64 --lr 1e-3\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bedbf47",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
